{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# append the path of the parent directory\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rudi/.local/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "/usr/lib/python3/dist-packages/pytz/__init__.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  match = re.match(\"^#\\s*version\\s*([0-9a-z]*)\\s*$\", line)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "import orjson\n",
    "import gzip\n",
    "\n",
    "from importlib import reload\n",
    "from lib import sketches, visualization_utils, encoders, ploting, pacha_sketch_new\n",
    "\n",
    "reload(pacha_sketch_new)\n",
    "from lib.pacha_sketch_new import ADTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/222/bank+marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/raw/bank_marketing/bank_full.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert month and day to day of the year indexed from 0\n",
    "df['date'] = df.apply(lambda row: (datetime.datetime.strptime(f\"{row['month']} {row['day']}\", \"%b %d\").timetuple().tm_yday - 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df[['poutcome', 'job', 'education', 'housing', 'loan', 'marital', 'duration', 'balance','campaign', 'age', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_parquet(\"data/clean/bank_marketing.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries and AD-Tree Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_parquet(\"data/clean/bank_marketing.parquet\")\n",
    "cat_cols = ['poutcome', 'job', 'education', 'housing', 'loan', 'marital']\n",
    "num_cols = ['duration', 'balance','campaign', 'age', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poutcome        4\n",
       "job            12\n",
       "education       4\n",
       "housing         2\n",
       "loan            2\n",
       "marital         3\n",
       "duration     1573\n",
       "balance      7168\n",
       "campaign       48\n",
       "age            77\n",
       "date          318\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_tree = ADTree()\n",
    "\n",
    "for col in cat_cols:\n",
    "    ad_tree.add_dimension(set(bank_df[col].unique().tolist()), name=col)\n",
    "\n",
    "ad_tree.save_to_file(\"sketches/ad_trees/bank_marketing.json\")\n",
    "print(ad_tree.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =bank_df\n",
    "col = 'balance'\n",
    "range_size = 0.1\n",
    "max_val = int(df[col].max())\n",
    "min_val = int(df[col].min())\n",
    "range_size = int((max_val - min_val) * range_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = random.randint(min_val, max_val-range_size)\n",
    "end = start + range_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(df: pd.DataFrame, num_cols: list, cat_cols: list, num_queries: int = 200, n_cat: int = 1, n_num: int = 1,\n",
    "                      decay_rate: float = 0.5, range_portion:float = 0.1, dataset_name: str = None, file_path: str = None) -> list:\n",
    "    # Exponentially decaying probability for each index - smaller decay_rate means faster decay\n",
    "    probs_cat = np.array([decay_rate**i for i in range(len(cat_cols))])\n",
    "    probs_cat = probs_cat / probs_cat.sum()\n",
    "\n",
    "    probs_num = np.array([decay_rate**i for i in range(len(num_cols))])\n",
    "    probs_num = probs_num / probs_num.sum()\n",
    "\n",
    "    queries = []\n",
    "    for i in range(num_queries):\n",
    "        filter_predicates = {}\n",
    "\n",
    "        picked_cat_cols = np.random.choice(cat_cols, size=n_cat, replace=False, p=probs_cat)\n",
    "        for col in picked_cat_cols:\n",
    "            val_counts = df[col].value_counts(normalize=True)\n",
    "            predicate = set(np.random.choice(val_counts.index, size=1, p=val_counts.values))\n",
    "            filter_predicates[col] = predicate\n",
    "\n",
    "        picked_num_cols = np.random.choice(num_cols, size=n_num, replace=False, p=probs_num)\n",
    "        for col in picked_num_cols:\n",
    "            max_val = int(df[col].max())\n",
    "            min_val = int(df[col].min())\n",
    "            range_size = int((max_val - min_val) * range_portion)\n",
    "            start = random.randint(min_val, max_val-range_size)\n",
    "            end = start + range_size\n",
    "            filter_predicates[col] = (start, end)\n",
    "\n",
    "        all_predicates = []\n",
    "        for col in df.columns:\n",
    "            if col in filter_predicates:\n",
    "                all_predicates.append(filter_predicates[col])\n",
    "            else:\n",
    "                all_predicates.append(\"*\")\n",
    "        queries.append(all_predicates)\n",
    "    \n",
    "    query_set = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"n_cat\": n_cat,\n",
    "        \"n_num\": n_num,\n",
    "        \"num_queries\": num_queries,\n",
    "        \"range_size\": range_size,\n",
    "        \"queries\": queries\n",
    "    }\n",
    "\n",
    "    if file_path is not None:\n",
    "        if file_path.endswith('.gz'):\n",
    "            with gzip.open(file_path, \"wb\") as f:\n",
    "                f.write(orjson.dumps(query_set))\n",
    "        else:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(orjson.dumps(query_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Type is not JSON serializable: set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21464/4204469538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbank_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_queries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bank_marketing_2_cols\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"queries/bank_marketing_2_cols.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_21464/853843222.py\u001b[0m in \u001b[0;36mgenerate_queries\u001b[0;34m(df, num_cols, cat_cols, num_queries, n_cat, n_num, decay_rate, range_portion, dataset_name, file_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Type is not JSON serializable: set"
     ]
    }
   ],
   "source": [
    "generate_queries(df=bank_df, num_cols=num_cols, cat_cols=cat_cols, num_queries=3, n_cat=1, n_num=1, dataset_name=\"bank_marketing_2_cols\", file_path=\"queries/bank_marketing_2_cols.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries(df=bank_df, num_cols=num_cols, cat_cols=cat_cols, num_queries=200, n_cat=2, n_num=2, dataset_name=\"bank_marketing_4_cols\", file_path=\"queries/bank_marketing_4_cols.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries with two predicates\n",
    "n_cat = 1\n",
    "n_num = 1\n",
    "\n",
    "range_size = 0.1\n",
    "\n",
    "predicates = {}\n",
    "\n",
    "cat_cols = np.random.choice(cat_cols, size=n_cat, replace=False, p=probs_cat)\n",
    "cat_predicates = {}\n",
    "for col in cat_cols:\n",
    "    val_counts = bank_df[col].value_counts(normalize=True)\n",
    "    predicate = set(np.random.choice(val_counts.index, size=1, p=val_counts.values))\n",
    "    predicates[col] = predicate\n",
    "\n",
    "# print(cat_predicates) \n",
    "\n",
    "num_cols = np.random.choice(num_cols, size=n_num, replace=False, p=probs_num)\n",
    "num_predicates = {}\n",
    "for col in num_cols:\n",
    "    max_val = bank_df[col].max()\n",
    "    min_val = bank_df[col].min()\n",
    "    range_size = int((max_val - min_val) * range_size)\n",
    "\n",
    "    start = random.randint(min_val, max_val-range_size)\n",
    "    end = start + range_size\n",
    "    predicates[col] = (start, end)\n",
    "\n",
    "print(predicates)\n",
    "\n",
    "final_predicates = []\n",
    "for col in bank_df.columns:\n",
    "    if col in predicates:\n",
    "        final_predicates.append(predicates[col])\n",
    "    else:\n",
    "        final_predicates.append(\"*\")\n",
    "        \n",
    "print(final_predicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'poutcome' in predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Retail\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/352/online+retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/raw/online_retail/online_retail.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['StockCode'].str.slice(0, 3)\n",
    "df['date'] = pd.to_datetime(df['InvoiceDate'], format=\"%d.%m.%Y %H:%M\").dt.strftime('%d.%m.%Y').rank(method='dense').astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = pd.DataFrame(df['CustomerID'].unique(), columns=['CustomerID'])\n",
    "df_customer['age'] = generate_bounded_normal(loc=35, scale=10, size=len(df_customer), low=18, high=76)\n",
    "df_customer['gender'] = np.random.choice(['m', 'f', 'd'], size=len(df_customer), replace=True, p=[0.48, 0.49, 03])\n",
    "df_merged = df.merge(df_customer, on='CustomerID', how='inner')\n",
    "df_merged[\"region\"] = df_merged['Country']\n",
    "df_merged['total'] = df_merged['Quantity'] * df_merged['UnitPrice']\n",
    "df_merged['total'] = df_merged['total'].abs().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df_merged[['region', 'gender', 'category', 'date', 'total', 'age']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_parquet(\"data/clean/online_retail.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folktables\n",
    "\n",
    "https://github.com/socialfoundations/folktables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column  | Description                                                                       |\n",
    "| ------- | --------------------------------------------------------------------------------- |\n",
    "| `SEX`   | **Sex** — 1 for male, 2 for female.                                               |\n",
    "| `RAC1P` | **Race** — Detailed race code (White, Black, Asian, etc.).                        |\n",
    "| `SCHL`  | **Educational attainment** — Highest degree or level of school completed.         |\n",
    "| `MAR`   | **Marital status** — E.g., married, divorced, widowed, never married.             |\n",
    "| `POBP`  | **Place of birth** — Numeric code for U.S. state or foreign country of birth.     |\n",
    "| `COW`   | **Class of worker** — Employment type (e.g., private, government, self-employed). |\n",
    "| `OCCP`  | **Occupation code** — Detailed job classification (4-digit code).                 |\n",
    "| `AGEP`  | **Age of person** — Age in years (0–99, top-coded at 99).                         |\n",
    "| `PWGTP` | **Person’s weight** — Statistical weight used to produce population estimates.    |\n",
    "| `PINCP` | **Total person income** — Total pre-tax income in the past 12 months.             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSIncome, generate_categories, adult_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2017', horizon='1-Year', survey='person')\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"SEX\", \"RAC1P\", \"SCHL\", \"MAR\", \"POBP\", \"COW\", \"OCCP\"]\n",
    "num_cols = [\"AGEP\", \"PWGTP\", \"PINCP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folk_df = ca_data[cat_cols + num_cols]\n",
    "folk_df.fillna(0, inplace=True)\n",
    "folk_df = folk_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folk_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folk_df.to_parquet(\"data/clean/acs_folktables.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_dataset(size, num_columns, mean, std_dev, output_path=None):\n",
    "    \"\"\"\n",
    "    Generates a dataset with the specified parameters where each column\n",
    "    contains integer values following a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): Number of rows in the dataset.\n",
    "    - num_columns (int): Number of columns in the dataset.\n",
    "    - mean (float): Mean of the normal distribution.\n",
    "    - std_dev (float): Standard deviation of the normal distribution.\n",
    "    - output_dir (str, optional): Directory to save the dataset in Parquet format. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Generated dataset as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Generate random data for each column\n",
    "    data = {\n",
    "        f\"d_{i}\": np.random.normal(loc=mean, scale=std_dev, size=size).astype(int)\n",
    "        for i in range(num_columns)\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to output directory if specified\n",
    "    if output_path:\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        print(f\"Dataset saved to {output_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_normal_dataset(size=100000, num_columns=3, mean=5000, std_dev=50, output_path=\"data/normal_3d_100k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/normal_3d_100k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Dimensions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bounded_normal(loc, scale, size, low, high):\n",
    "    result = []\n",
    "    while len(result) < size:\n",
    "        samples = np.random.normal(loc=loc, scale=scale, size=size)\n",
    "        valid_samples = samples[(samples >= low) & (samples <= high)]\n",
    "        result.extend(valid_samples.astype(int))\n",
    "    return np.array(result[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_dataset(size, output_path=None):\n",
    "\n",
    "    regions = [\n",
    "        \"Baden-Württemberg\", \"Bavaria\", \"Berlin\", \"Brandenburg\", \"Bremen\", \n",
    "        \"Hamburg\", \"Hesse\", \"Lower Saxony\", \"Mecklenburg-Vorpommern\", \n",
    "        \"North Rhine-Westphalia\", \"Rhineland-Palatinate\", \"Saarland\", \n",
    "        \"Saxony\", \"Saxony-Anhalt\", \"Schleswig-Holstein\", \"Thuringia\"\n",
    "    ]\n",
    "    gender = [\"m\",\"f\",\"d\"]\n",
    "    product_category = [chr(i) for i in range(97, 123)]\n",
    "\n",
    "    p_product_category = np.random.zipf(1.5, len(product_category))\n",
    "    p_product_category = p_product_category / np.sum(p_product_category)\n",
    "\n",
    "    reference_dist = pd.read_parquet(\"data/reference_dist.parquet\")\n",
    "\n",
    "    ages_array = generate_bounded_normal(loc=35, scale=10, size=size, low=18, high=76)\n",
    "\n",
    "    data = {\n",
    "        \"region\": np.random.choice(regions, size=size, replace=True),\n",
    "        \"gender\": np.random.choice(gender, size=size, replace=True, p=[0.48, 0.49, 03]),\n",
    "        \"category\": np.random.choice(product_category, size=size, replace=True, p=p_product_category),\n",
    "        \"date\": np.random.choice(reference_dist['date'], size=size, replace=True),\n",
    "        \"total\": np.random.choice(reference_dist['total'], size=size, replace=True),\n",
    "        \"age\": ages_array      \n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to output directory if specified\n",
    "    if output_path:\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        print(f\"Dataset saved to {output_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_fake_dataset(size=200000, output_path=\"data/paper_example_200k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['age'], bins=30, kde=True, color='blue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random ages with a normal distribution\n",
    "ages_array = np.random.normal(loc=35, scale=10, size=1000).astype(int)\n",
    "\n",
    "# Clip the ages to ensure they fall within the range of 18 to 76\n",
    "ages_array = np.clip(ages_array, 18, 76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-Commerce Sales Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/raw/amazon_sale_report.csv\", low_memory=False)\n",
    "df.to_parquet(\"data/raw/amazon_sale_report.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/raw/amazon_sale_report.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_subset = df[['Date', 'Status',\n",
    "       'Style', 'Category', 'Size',\n",
    "       'Qty', 'Amount',\n",
    "       'ship-state']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_subset['total'] = df_column_subset['Amount'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_subset['date'] = pd.to_datetime(df_column_subset['Date'], format='%m-%d-%y').rank(method='dense').astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_subset[['date', 'total']].to_parquet(\"data/raw/reference_dist.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_subset.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
